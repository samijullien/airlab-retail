{"cells":[{"cell_type":"markdown","metadata":{},"source":"# Experiments\n\nHere, we compare standard baselines to imitation learning and RL-based methods. Please read the readme first."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"#First, we import our libraries and define the neural net model used for DQN. \n#We specify store hyperparameters then build the training function for our agent\n\nimport torch.nn as nn\nimport torch\nfrom retail import retail\nimport torch.distributions as d\nimport torch.nn.functional as F\nimport numpy as np\nbucket_customers = torch.tensor([800.,400.,500.,900.])\n\nfrom rlpyt.algos.dqn.dqn import DQN\nfrom rlpyt.agents.pg.categorical import CategoricalPgAgent\nfrom rlpyt.agents.dqn.dqn_agent import DqnAgent\nfrom rlpyt.samplers.serial.sampler import SerialSampler\nfrom rlpyt.agents.dqn.dqn_agent import DqnAgent\nfrom rlpyt.runners.minibatch_rl import MinibatchRlEval\nfrom rlpyt.utils.logging.context import logger_context\nfrom rlpyt.samplers.parallel.cpu.sampler import CpuSampler\nfrom rlpyt.algos.pg.ppo import PPO\nfrom rlpyt.agents.pg.gaussian import GaussianPgAgent\ntorch.set_default_tensor_type(torch.FloatTensor)\n\nclass DQNModel(torch.nn.Module):\n    def __init__(\n            self,\n            input_size,\n            conv_size,\n            hidden_sizes,\n            n_kernels,\n            output_size=None,  \n            nonlinearity=torch.nn.SELU,  \n            ):\n        super().__init__()\n        self._conv_size = conv_size\n        self._char_size = input_size-conv_size\n        self.normL = torch.nn.LayerNorm(input_size)\n        self._transf_in_size = input_size-conv_size+n_kernels\n        self.activ = nn.SELU()\n        if isinstance(hidden_sizes, int):\n            hidden_sizes = [hidden_sizes]\n        hidden_layers = [torch.nn.Linear(n_in, n_out) for n_in, n_out in\n            zip([self._transf_in_size] + hidden_sizes[:-1], hidden_sizes)]\n        sequence = list()\n        for layer in hidden_layers:\n            sequence.extend([layer, nonlinearity(), torch.nn.LayerNorm(layer.out_features)])\n        if output_size is not None:\n            last_size = hidden_sizes[-1] if hidden_sizes else input_size\n            sequence.append(torch.nn.Linear(last_size, output_size))\n        self.convs = torch.nn.Conv1d(1, out_channels = n_kernels, kernel_size = 1)\n        self.model = torch.nn.Sequential(*sequence)\n        self._output_size = (hidden_sizes[-1] if output_size is None\n            else output_size)\n\n    def forward(self, o, action, reward):\n        oprime = self.normL(o)\n        stock, characteristics = o.split([self._conv_size,self._char_size],-1)\n        stock_summary = self.convs(stock.view(-1,1,self._conv_size)).mean(2)\n        if len(characteristics.shape)>2:\n            stock_summary.unsqueeze_(1)\n        summarized_input = torch.cat((self.activ(stock_summary), characteristics),-1)\n        out = self.model(summarized_input)\n        return out\n    @property\n    def output_size(self):\n        return self._output_size\n\nkwargsStore= {'assortment_size': 1, 'utility_weights' :{'alpha': 1., 'beta': 1., 'gamma': 1.},\n             'max_stock': 1000, 'forecastVariance' :0.000, 'horizon': 730, 'lead_time': 1}\nkwargsModel = {'input_size': 1006, 'hidden_sizes': [100,200, 500, 1000],  'conv_size': 1000, 'n_kernels': 50}\n\ndef build_and_train(run_ID=1, cuda_idx=None, env_kwargs = None, mid_batch_reset = False, \n                    n_parallel = 1, model_kwargs=None, initial_model_state_dict = None):\n    affinity = dict(workers_cpus=list(range(n_parallel)))\n    sampler = CpuSampler(\n        EnvCls= retail.StoreEnv,\n        env_kwargs = env_kwargs,\n        eval_env_kwargs = env_kwargs,\n        batch_T=20,\n        batch_B=2,\n        max_decorrelation_steps=10,\n        eval_n_envs=1,\n        eval_max_steps=int(20e4),\n        eval_max_trajectories=24\n    )\n    algo = DQN(learning_rate=5e-5,discount=.99, replay_ratio=8, batch_size = 16) \n    agent = DqnAgent(ModelCls=DQNModel, model_kwargs=model_kwargs)\n    runner = MinibatchRlEval(\n        algo=algo,\n        agent=agent,\n        sampler=sampler,\n        n_steps=10e4,\n        log_interval_steps=1e3,\n        affinity=affinity )\n    \n    name = \"dqn\"\n    log_dir = \"experiments\"\n    config = None\n    runner.train()\n    dic = agent.state_dict()\n    torch.save(dic,'dqn-lt1.pt')"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Next, we simply train our DQN agent.\n","\n","build_and_train(\n","        run_ID=10,\n","        env_kwargs = kwargsStore,\n","        model_kwargs = kwargsModel)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# We define the network for PPO, then its parameter and training function\nclass NewPPO(torch.nn.Module):\n    def __init__(\n            self,\n            input_size,\n            conv_size,\n            hidden_sizes,\n            n_kernels,\n            output_size=None,  \n            nonlinearity=torch.nn.SELU,  \n            ):\n        super().__init__()\n        self._conv_size = conv_size\n        self._char_size = input_size-conv_size\n        self.normL = torch.nn.LayerNorm(input_size)\n        self._transf_in_size = input_size-conv_size+n_kernels\n        self.activ = nn.SELU()\n        if isinstance(hidden_sizes, int):\n            hidden_sizes = [hidden_sizes]\n        hidden_layers = [torch.nn.Linear(n_in, n_out) for n_in, n_out in\n            zip([self._transf_in_size] + hidden_sizes[:-1], hidden_sizes)]\n        sequence = list()\n        for layer in hidden_layers:\n            sequence.extend([layer, nonlinearity(), torch.nn.LayerNorm(layer.out_features)])\n        if output_size is not None:\n            last_size = hidden_sizes[-1] if hidden_sizes else input_size\n            sequence.append(torch.nn.Linear(last_size, output_size))\n        self.convs = torch.nn.Conv1d(1, out_channels = n_kernels, kernel_size = 1)\n        self.model = torch.nn.Sequential(*sequence)\n        self._output_size = (hidden_sizes[-1] if output_size is None\n            else output_size)\n\n    def forward(self, o, action, reward):\n        oprime = self.normL(o)\n        stock, characteristics = o.split([self._conv_size,self._char_size],-1)\n        stock_summary = self.convs(stock.view(-1,1,self._conv_size)).mean(2)\n        if len(characteristics.shape)>2:\n            stock_summary.unsqueeze_(1)\n        summarized_input = torch.cat((self.activ(stock_summary), characteristics),-1)\n        out = self.model(summarized_input).squeeze()\n        return out.split(1, dim = -1)\n    @property\n    def output_size(self):\n        return self._output_size\n\n    \nkwargsStore= {'assortment_size': 1, 'symmetric_action_space': True,\n             'max_stock': 1000, 'forecastVariance' :0.000, 'horizon': 730, 'lead_time': 1}\nkwargsModel = {'input_size': 1006, 'hidden_sizes': [100,200,100], \n               'output_size': 3, 'conv_size': 1000, 'n_kernels': 50}\n\ndef build_and_train(run_ID=1, cuda_idx=None, env_kwargs = None, \n                    mid_batch_reset = False, n_parallel = 1,model_kwargs=None):\n    affinity = dict(workers_cpus=list(range(n_parallel)))\n    sampler = CpuSampler(\n        EnvCls= retail.StoreEnv,\n        env_kwargs = env_kwargs,\n        eval_env_kwargs = env_kwargs,\n        batch_T=20,\n        batch_B=1,\n        max_decorrelation_steps=20,\n        eval_n_envs=100,\n        eval_max_steps=int(10e4),\n        eval_max_trajectories=20)\n    algo = PPO(learning_rate = 1e-5, discount=.99, value_loss_coeff=.2, \n               normalize_advantage = True, ratio_clip = .5)\n    agent = GaussianPgAgent(ModelCls=NewPPO, model_kwargs=kwargsModel)\n    runner = MinibatchRlEval(\n        algo=algo,\n        agent=agent,\n        sampler=sampler,\n        n_steps=1e5,\n        log_interval_steps=1e3,\n        affinity=affinity\n    )\n    name = \"pg\"\n    log_dir = \"experiments\"\n    config = None\n    runner.train()\n    dic = agent.state_dict()\n    torch.save(dic,'ppo-lt1.pt')-ppo-lt1.pt')"},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["#We then train our PPO based agent\n","build_and_train(\n","        run_ID=11,\n","        env_kwargs = kwargsStore,\n","        model_kwargs = kwargsModel)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#We collect results for the forecast based policy\n","rewards_forecast_order = []\n","for i in range(100):\n","    ra = []\n","    done = False\n","    kwargsStore= {'assortment_size': 250, 'lead_time': 1,\n","             'max_stock': 1000, 'seed': i, 'forecastVariance' :0.000, 'horizon': 500}\n","\n","    store = retail.StoreEnv(**kwargsStore)\n","    while not (done):\n","        customers = bucket_customers.sum()\n","        p = store.forecast.squeeze()\n","        std = torch.sqrt(customers*p+(1-p))\n","        order = F.relu(3*std+store.forecast.squeeze()*customers-store.get_full_inventory_position()).round()\n","        obs = store.step(order.numpy())\n","        ra.append(obs[1])\n","        done = obs[2]\n","    rewards_forecast_order.append(torch.stack(ra).mean())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Same for the fixed order rate\n","\n","rewards_sq_policy = []\n","for i in range(100):\n","    ra = []\n","    done = False\n","    kwargsStore= {'assortment_size': 250, 'lead_time': 1,\n","             'max_stock': 1000, 'seed': i, 'forecastVariance' :0.000, 'horizon': 500}\n","    store = retail.StoreEnv(**kwargsStore)\n","    while not (done):\n","        order = F.relu(2*store.assortment.base_demand*bucket_customers.max()-store.get_full_inventory_position()).round()\n","        obs = store.step(order.numpy())\n","        ra.append(obs[1])\n","        done = obs[2]\n","    rewards_sq_policy.append(torch.stack(ra).mean())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"#Now, we recover our DQN network and collect results\n\nkwargsModel = {'input_size': 1006, 'hidden_sizes': [100,200, 500, 1000],  'conv_size': 1000, 'n_kernels': 50}\nnewDic= torch.load('dqn-lt1.pt',map_location=torch.device('cpu'))\nmodel = DQNModel(**kwargsModel)\nmodel.load_state_dict(newDic['model'])\n\n\nrewards_dqn = []\n\nfor i in range(100):\n    print(i)\n    ra = []\n    done = False\n    kwargsStore= {'assortment_size': 250, 'lead_time': 1,\n             'max_stock': 1000, 'seed': i, 'forecastVariance' :0.000, 'horizon': 500}\n\n    store = retail.StoreEnv(**kwargsStore)\n    while not (done):\n        mu = model(store.get_obs(),5, 10).max(1)[1]\n        obs = store.step(mu.detach().numpy())\n        ra.append(obs[1])\n        done = obs[2]\n\n    rewards_dqn.append(torch.stack(ra).mean())"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"#Same for PPO\n\nkwargsModel = {'input_size': 1006, 'hidden_sizes': [100,200,100], \n               'output_size': 3, 'conv_size': 1000, 'n_kernels': 50}\nnewDic= torch.load('ijcai_genplan-ppo-lt1.pt',map_location=torch.device('cpu'))\nmodel = NewPPO(**kwargsModel)\nmodel.load_state_dict(newDic)\nrewards_ppo = []\nfor i in range(100):\n    ra = []\n    done = False\n    kwargsStore= {'assortment_size': 250, 'lead_time': 1,'symmetric_action_space': True,\n             'max_stock': 1000, 'seed': i, 'forecastVariance' :0.000, 'horizon': 500}\n\n    store = retail.StoreEnv(**kwargsStore)\n    while not (done):\n        mu, sigma, value = model(store.get_obs(),5, 10)\n        obs = store.step(mu.detach().numpy())\n        ra.append(obs[1])\n        done = obs[2]\n    rewards_ppo.append(torch.stack(ra).mean())\n    print(torch.stack(ra).mean())"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#We now move to imitation learning. We collect samples from a policy\n","\n","\n","actionList = []\n","observations = []\n","rewards = []\n","#Here, the seed ensure that we do not mix train and test\n","for i in range(101,1101):\n","    ra = []\n","    done = False\n","    kwargsStore= {'assortment_size': 1, 'lead_time': 1,\n","             'max_stock': 1000, 'seed': i, 'forecastVariance' :0.000, 'horizon': 500}\n","    store = retail.StoreEnv(**kwargsStore)\n","    while not (done):\n","        observations.append(store.get_obs())\n","        customers = bucket_customers.sum()\n","        p = store.forecast.squeeze()\n","        std = torch.sqrt(customers*p+(1-p))\n","        order = F.relu(3*std+store.forecast.squeeze()*customers-store.get_full_inventory_position()).round()\n","        obs = store.step(order.numpy())\n","        ra.append(obs[1])\n","        actionList.append(order)\n","        done = obs[2]\n","    rewards += ra"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","class ClassicDataSet(Dataset):\n","    def __init__(self, x, y, r):\n","        self._x = x\n","        self._y = y\n","        self._r = r\n","        self._len = x.shape[0]\n","        \n","    def __len__(self):\n","        return(self._len)\n","    \n","    def __getitem__(self, index):\n","        return(self._x[index], self._y[index], self._r[index])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y = torch.stack(actionList)\n","x = torch.stack(observations)\n","r = torch.stack(rewards)    \n","action_reaction = ClassicDataSet(x, y, r)\n","\n","data_loader = DataLoader(action_reaction, batch_size=128,\n","                            shuffle=True, num_workers=2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Now, we simply need to replicate this policy. The neural network used is the same as PPO\n","kwargsPPO = {'input_size': 1006, 'hidden_sizes': [100,200,100], \n","               'output_size': 3, 'conv_size': 1000, 'n_kernels': 50}\n","newNet = NewPPO(**kwargsPPO)\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(newNet.parameters(), lr=1e-3)\n","for epoch in range(20):\n","    # Training\n","    s = 0\n","    s2 = 0\n","    for data, target, reward in (data_loader):\n","        net_out = newNet(data,0,0)\n","        loss = criterion(net_out[0], target)\n","        s += loss\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","    print(s)\n","name = 'imitationLearning.pt'\n","torch.save(newNet.state_dict(),name)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"#We now collect rewards from the imitating neural network\n\nrewards_imitation = []\nfor i in range(100):\n    print(i)\n    ra = []\n    done = False\n    kwargsStore= {'assortment_size': 250, 'lead_time': 1,\n             'max_stock': 1000, 'seed': i, 'forecastVariance' :0.000, 'horizon': 500}\n\n    store = env_rlpyt.StoreEnv(**kwargsStore)\n    while not (done):\n        mu, sigma, value = newNet(store.get_obs(),5, 10)\n        obs = store.step(mu.detach().numpy())\n        ra.append(obs[1])\n        done = obs[2]\n    rewards_imitation.append(torch.stack(ra).mean())\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"We can plot a histogram\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.kdeplot(torch.stack(rewards_dqn).numpy())"},{"cell_type":"markdown","metadata":{},"source":"We can now save the results. It's easy to compute the mean or the std over the obtained sample."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"baseline_csv = torch.stack((torch.stack(rewards_sq_policy), torch.stack(rewards_forecast_order))).numpy()\nrl_csv = torch.stack((torch.stack(rewards_dqn), torch.stack(rewards_ppo), torch.stack(rewards_imitation))).numpy()\nnp.savetxt(\"baseline_results.csv\", baseline_csv, delimiter=\",\")\nnp.savetxt(\"rl_results.csv\", rl_csv, delimiter=\",\")\n\n"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}